{"posts":[{"title":"驱虫市场电商分析案例👻","content":" 1.基本分析逻辑 逐步缩进， 先看总的，再看每一类目 1.1分析流程 1.2产品结构 - 波士顿矩阵 1.3市场类别 评判市场和品牌的发展趋势和增长情况,从宏观到微观,从大市场到细分市场: 互联网产品由关注用户增量到用户存量,判断产品或市场是用户增量还是存量,只需要判断有新的需求出现即可:增量市场:从无到有,以前关注哪些需求没有被满足,快速迭代抢占市场,考虑最多的不是用户体验. 流量=新增客户.例如:智能手机潮开始时的市场.小米面对的是增量. 存量市场:从有到优,现在关注如何更好的满足需求,考虑更多的是用户体验. 产品价值=新体验-旧体验-替换成本,新体验没有突破性大幅增加,产品价值很难实现. 流量=用户时间(停留时间越久,利益价值越大). 例如:现在人手一台智能手机,小米面对存量市场,如何让需要换手机的用户换成小米,从有到优. 创新:想要用产品价值撬动一个用户,同纬度竞争别家的先发优势门槛太高,如果别家体量很大,基本可以放弃; 创新可能就是剩下的活路,而面对互联网的高速发展,线下需求基本都被互联网化,切入点可能就转移到细分市 场. 例如:微信QQ是社交领域的霸主,陌陌探探在陌生人社交上也分了一杯羹,这些已存在的需求,没有被充分实现,也 算增量市场 1.4产品生命周期 1.5处理项目需求的基本思路 了解项目公司的背景和对接人员情况 公司的产品结构,市场环境,对接人的角色和权利等级等 沟通明确实际的项目需求 团队内部理解项目需求 和业务方沟通需求:从业务的角度理解需求可能的解决方案 优化项目需求 和业务核对项目需求 根据项目需求梳理分析思路:每一步分析的目标,需要的数据支持,反复优化 确定分析工具和人员配置,进行数据分析 撰写分析结论和方案 1.6 项目需求例子 问题:销售额下降,怎么办?(这个问题太大,方法也很多:优化老客户,扩大流量,提升转化率) 了解涉及项目相关的所有的业务部门的需求,逻辑,问题点 拆分:销售额=流量转化率客单价 待沟通部门:营销部门(活动),推广部门(流量),客服,售后,供应链 营销:精准营销(找到高价值客户),客户行为分析(响应效果),组合营销(购物篮)推广:竞价排名,买广告位,点击付费(需要很强的经验) 退款和评论分析:优化产品,优化服务质量 沟通之前出想法,沟通之后优化,确认项目需求 数据收集:确认每一步需求的数据(可能用到爬虫) 2.项目背景及产品架构、数据说明 客户介绍:拜耳官方旗舰店(拜耳公司,总部位于德国的勒沃库森,在六大洲的200个地 点建有750家生产厂;拥有120,000名员工及350家分支机构,几乎遍布世界各国.高分 子,医药保健,化工以及农业是公司的四大支柱产业.公司的产品种类超过10000种) 客户需求:拜耳官方旗舰店 寻求市场增长点 产品架构: 数据说明： 3.驱虫市场潜力分析 import numpy as np import pandas as pd import matplotlib.pyplot as plt import seaborn as sns %matplotlib inline ","link":"https://416426.github.io/qu-chong-shi-chang-dian-shang-fen-xi-an-li/"},{"title":"爬虫","content":"爬虫的概念 模拟客户端发送网络请求 获取响应 按照规则提取数据的程序 浏览器请求 url 在chrome -- 点检查 -- 点network url = 请求协议+网站域名+资源的路径+参数 浏览器请求url地址 当前url对应的响应+js+css+图片 ------&gt; elements中的内容 爬虫请求url地址 当前的url对应的响应 elements的内容和爬虫获取的url地址响应不同，爬虫需要以当前url地址对应的响应为准提取数据 当前URL地址对应的响应在 从network中找到当前url地址 点击response 页面右键显示网页源码 HTTP HTTPS HTTP：超文本传输协议 明文形式传输 效率更高，但不安全 HTTPS： HTTP+ SSL（安全套接字层） 传输前数据先加密， 之后解密获取内容 效率较低，但是安全 HTTP协议及请求 General - 请求参数 url地址 请求方式 状态码 Response Headers - 响应头 Request Headers - 请求头 点 View source 请求行 -- get -请求方式 get请求和post请求的区别 get请求没有请求体， post有，get请求把数据放到url地址中 post请求常用与登陆注册 post请求携带的数据量比get请求大，多，常用语传输打文本的时候 请求头 User-Agent： 用户代理： 对方服务器可以通过user_agent知道当前请求对方资源的是什么浏览器 ​ 若果我们需要模拟手机版浏览器发送请求 ，对应的就需要把user_agent改成手机版 Cookie: 用于存储用户信息，每次请求携带上发送给对方浏览器 要获取登陆后才能访问的页面 对方服务器回通过cookie判断我们是不是一个爬虫 name ----- value 请求体 携带数据 get请求没有请求体 post有请求体 HTTP协议--响应 响应头 Set-Cookie： 对方服务通过该字段设置cookie到本地 响应体 url地址对应的响应 Query String Parameters - 请求参数 requests模块 安装 --- pip install requests 发送get post请求 获取响应 response = requests.get(url) # 发送get请求， 请求url对应的响应 response = requests.post(url, data = {请求题的字典}) ## 发送post请求， 请求的url地址对应的响应 response的方法 ​ response.text 该方式常出现乱码, 原因是编码解码方式不一样，出现乱码使用response.encoding = &quot;utf-8&quot; response.content.decode() 把响应的二进制字节流转化为str类型 response.request.url # 发送请求的url地址 response.url # response响应的url地址 response.requset.headers #请求头 response.headers # 响应头 获取网页源码的正确打开方式（一定可以获取到） response.content.decode() response.content.decode(“gbk”) response.text 直接使用response.content.decode()获取，获取不到的话使用response.content.decode(“gbk”)国标码尝试，如果都获取不到response.text一定可以拿到 发送带header的请求 headers = {&quot;User-Agent&quot;:&quot; Mozilla/5.0 (iPhone; CPU iPhone OS 13_2_3 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/13.0.3 Mobile/15E148 Safari/604.1&quot; &quot;Referer&quot;:&quot; https://fanyi.baidu.com/?aldtype=16047&quot;} response = sequests.get(url, headers=headers) 当请求网页发现带一个键值对依然不可以请求成功，可以尝试带更多的键值对，这是根据对方服务器的一个判断进行尝试，可以把除了cookie的全部键值对都带这进行尝试，如果依然不可以，把cookie也带着请求 ---- 大部分情况User-Agent足以 使用超时参数 requests.get(url, headers=headers, timeout=3) timeout=3 --- 3秒内必须返回响应，否则回报错 ​ ","link":"https://416426.github.io/pa-chong/"}]}